# Official transformers-neuronx API Implementation

## ‚úÖ Implemented Correct AWS Neuron Documentation API

Based on the official AWS Neuron documentation, I've implemented the **correct transformers-neuronx API** that should properly distribute the Mistral 7B model across both Neuron cores using tensor parallelism.

## Key Changes Applied ‚úÖ

### **1. ‚úÖ Correct Model Class**
**Before**: `MistralForSampling` with `NeuronConfig` and `GQA.SHARD_OVER_HEADS`
**After**: `NeuronAutoModelForCausalLM` (official AutoModel class)

### **2. ‚úÖ Proper Configuration Parameters**
**Before**: Complex configuration with separate `NeuronConfig`
**After**: Direct parameters in `from_pretrained()`:

```python
from transformers_neuronx import NeuronAutoModelForCausalLM

model = NeuronAutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    batch_size=BATCH_SIZE,                    # Batch size for compilation
    n_positions=MAX_LENGTH,                   # Maximum sequence length  
    tp_degree=TENSOR_PARALLEL_SIZE,           # ‚úÖ Tensor parallelism degree (2 cores)
    amp='bf16',                               # Use bfloat16 for better performance
    context_length_estimate=min(512, MAX_LENGTH//2),  # Context encoding optimization
    trust_remote_code=True
)
model.to_neuron()  # ‚úÖ Compile for Neuron
```

### **3. ‚úÖ Correct Inference API**
**Before**: `model.sample()` with custom parameters
**After**: Standard HuggingFace `model.generate()` API:

```python
with torch.inference_mode():
    generated_ids = model.generate(
        input_ids,
        max_new_tokens=request.max_tokens,
        temperature=request.temperature,
        top_p=request.top_p,
        top_k=request.top_k,
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        repetition_penalty=request.repetition_penalty
    )
```

## Official AWS Documentation Pattern ‚úÖ

### **From AWS Neuron Documentation:**
```python
from transformers_neuronx import NeuronAutoModelForCausalLM

model = NeuronAutoModelForCausalLM.from_pretrained(
    'gpt2',                      # Model checkpoint
    batch_size=1,                # Batch size for compilation
    n_positions=128,             # Maximum sequence length
    tp_degree=2,                 # ‚úÖ Tensor parallelism across 2 NeuronCores
    amp='f16',                   # Mixed precision
    context_length_estimate=64,  # Context encoding optimization
)
model.to_neuron()  # Load/compile the model
```

## Key Benefits of This Approach ‚úÖ

### **1. ‚úÖ Official API Support**
- **Uses documented API** from AWS Neuron team
- **Follows best practices** from official documentation
- **Guaranteed compatibility** with transformers-neuronx

### **2. ‚úÖ Proper Tensor Parallelism**
- **`tp_degree=2`** explicitly sets tensor parallelism
- **Distributes model across 2 NeuronCores** automatically
- **No manual sharding configuration** needed

### **3. ‚úÖ HuggingFace Compatibility**
- **Standard `generate()` API** for inference
- **Compatible with existing code** patterns
- **Supports all standard generation parameters**

### **4. ‚úÖ Optimized Performance**
- **`amp='bf16'`** for better performance than float32
- **`context_length_estimate`** optimizes context encoding
- **`to_neuron()`** compiles for optimal Neuron execution

## Expected Behavior ‚úÖ

### **Startup Logs Should Show:**
```
üöÄ Loading Mistral model with transformers-neuronx optimization...
üîß Loading model with NeuronAutoModelForCausalLM...
‚öôÔ∏è Compiling model for Neuron (this may take several minutes)...
‚úÖ Optimized Neuron model loaded successfully
üìä Model configuration:
   - Batch size: 1
   - Tensor parallel degree: 2
   - Neuron cores: 2
   - Context length: 4096
   - Precision: bfloat16
   - Context estimate: 512
```

### **Inference Logs Should Show:**
```
üöÄ Using optimized transformers-neuronx generation
```

### **Memory Distribution:**
- **Core 0**: ~8GB (half of model via tensor parallelism)
- **Core 1**: ~8GB (half of model via tensor parallelism)
- **No more 15.938GB single-core allocation!**

## Technical Details ‚úÖ

### **Tensor Parallelism with tp_degree=2:**
- **Automatically shards** model weights across 2 NeuronCores
- **Distributes attention heads** (Mistral 7B has 32 heads, divisible by 2)
- **Enables collaborative computation** across cores
- **Reduces memory per core** from 16GB to ~8GB

### **NeuronAutoModelForCausalLM Benefits:**
- **Automatic model detection** - no need to specify MistralForSampling
- **Built-in optimizations** for Neuron hardware
- **Standard HuggingFace interface** for generation
- **Proper tensor parallelism handling**

### **Compilation Process:**
- **`model.to_neuron()`** compiles the model for Neuron
- **Applies tensor parallelism** automatically based on tp_degree
- **Optimizes memory layout** across cores
- **Creates efficient execution graphs**

## Files Updated ‚úÖ

### **1. ‚úÖ images/neuron-dlc/neuron_server.py**
- Updated `load_optimized_neuron_model()` with official API
- Updated inference to use `model.generate()`
- Added proper error handling and logging

### **2. ‚úÖ images/neuron-inferentia/neuron_server.py**
- Updated `load_optimized_neuron_model()` with official API
- Updated inference to use `model.generate()`
- Added proper error handling and logging

## Validation Against Documentation ‚úÖ

### **‚úÖ Matches Official Pattern:**
- Uses `NeuronAutoModelForCausalLM` ‚úÖ
- Uses `tp_degree=2` parameter ‚úÖ
- Uses `model.to_neuron()` compilation ‚úÖ
- Uses standard `generate()` API ‚úÖ

### **‚úÖ Follows Best Practices:**
- Uses `amp='bf16'` for performance ‚úÖ
- Sets `context_length_estimate` for optimization ‚úÖ
- Proper batch_size and n_positions configuration ‚úÖ
- Trust remote code for model loading ‚úÖ

## Expected Results ‚úÖ

### **1. ‚úÖ Proper Tensor Parallelism**
- Model distributed across both Neuron cores
- Memory usage ~8GB per core instead of 15.938GB on one core
- No more `RESOURCE_EXHAUSTED` errors

### **2. ‚úÖ Optimized Performance**
- Uses official transformers-neuronx optimizations
- Proper Neuron compilation with `to_neuron()`
- Better inference speed with bfloat16 precision

### **3. ‚úÖ Standard Interface**
- Compatible with HuggingFace generate() API
- Supports all standard generation parameters
- Easy to integrate with existing code

## Next Steps ‚úÖ

### **1. Rebuild Images:**
```bash
cd images/neuron-dlc && ./build.sh latest 820537372947.dkr.ecr.us-west-2.amazonaws.com
cd images/neuron-inferentia && ./build.sh latest 820537372947.dkr.ecr.us-west-2.amazonaws.com
```

### **2. Deploy Updated Images:**
```bash
kubectl rollout restart deployment neuron-mistral-7b-dlc
```

### **3. Monitor Startup:**
```bash
kubectl logs -l app=neuron-mistral-7b-dlc -f | grep -E "(NeuronAutoModelForCausalLM|tensor parallel|Neuron cores)"
```

### **4. Test Inference:**
```bash
kubectl port-forward svc/neuron-mistral-7b-dlc 8000:8000
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What is machine learning?", "max_tokens": 100}'
```

## Summary ‚úÖ

The implementation now follows the **exact pattern from the official AWS Neuron documentation**:

1. ‚úÖ **Uses `NeuronAutoModelForCausalLM`** - Official AutoModel class
2. ‚úÖ **Uses `tp_degree=2`** - Proper tensor parallelism parameter
3. ‚úÖ **Uses `model.to_neuron()`** - Official compilation method
4. ‚úÖ **Uses `model.generate()`** - Standard HuggingFace inference API
5. ‚úÖ **Uses `amp='bf16'`** - Optimized precision for performance

**üéâ This implementation should properly distribute the Mistral 7B model across both Neuron cores, resolving the memory allocation issue that was causing the 15.938GB single-core limit!**
