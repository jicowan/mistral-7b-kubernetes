# Accelerate Dependency Fix

## Issue Resolved âœ…

**Error**: `Using a device_map requires accelerate. You can install it with pip install accelerate`

**Root Cause**: Using `device_map="cpu"` in model loading requires the `accelerate` library, which wasn't properly available or had version conflicts.

## Fixes Applied âœ…

### **Fix #1: Updated Accelerate Installation**

#### **Before**:
```dockerfile
RUN pip install \
    accelerate==0.24.1
```

#### **After**:
```dockerfile
RUN pip install --upgrade \
    accelerate>=0.24.1
```

**Benefits**:
- âœ… **Latest accelerate version** with bug fixes
- âœ… **Flexible versioning** allows compatible updates
- âœ… **Proper installation** in container environment

### **Fix #2: Removed device_map Dependency**

#### **Before (Problematic)**:
```python
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="cpu",  # Requires accelerate
    offload_folder="/tmp/model_offload",  # Also requires accelerate
)
```

#### **After (Fixed)**:
```python
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    low_cpu_mem_usage=True,
    trust_remote_code=True
    # Removed device_map and offload_folder
)
# Explicitly move to CPU after loading
model = model.to('cpu')
```

**Benefits**:
- âœ… **No accelerate dependency** for basic CPU loading
- âœ… **Explicit device placement** with `.to('cpu')`
- âœ… **Simpler loading process** with fewer dependencies
- âœ… **Better compatibility** across different environments

### **Fix #3: Updated CPU Fallback Function**

#### **Applied Same Fix**:
```python
def load_cpu_fallback_model():
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True,
        trust_remote_code=True
        # Removed device_map="cpu"
    )
    model = model.to('cpu')  # Explicit CPU placement
```

### **Fix #4: Removed Deprecated Environment Variable**

#### **Before**:
```yaml
env:
- name: TRANSFORMERS_CACHE
  value: "/tmp/transformers_cache"  # Deprecated
- name: HF_HOME
  value: "/tmp/hf_home"
```

#### **After**:
```yaml
env:
- name: HF_HOME
  value: "/tmp/hf_home"  # Modern approach
- name: TOKENIZERS_PARALLELISM
  value: "false"
```

**Benefits**:
- âœ… **No deprecation warnings** in logs
- âœ… **Modern HF_HOME approach** for cache management
- âœ… **Cleaner environment** configuration

## Expected Behavior Now âœ…

### **Success Path**:
```
INFO:     Started server process [8]
INFO:     Waiting for application startup.
ğŸš€ Starting memory-efficient Neuron compilation with detailed debugging...
âœ… XLA modules imported successfully
ğŸ“Š Memory at start: System=2.34GB
ğŸ“ Loading tokenizer for mistralai/Mistral-7B-Instruct-v0.3...
âœ… Tokenizer loaded successfully (standard method)
ğŸ“Š Memory at tokenizer_loaded: System=2.35GB
ğŸ”„ Loading model on CPU with memory optimization...
âœ… Model loaded on CPU successfully
ğŸ“Š Memory at model_loaded_cpu: System=9.12GB
```

### **CPU Fallback Path**:
```
âŒ Failed to move model to XLA device: [Neuron memory error]
ğŸ”„ Loading CPU fallback model...
ğŸ“ Loading tokenizer for mistralai/Mistral-7B-Instruct-v0.3...
âœ… Tokenizer loaded successfully (standard method)
âœ… CPU fallback model loaded successfully (float32)
âš ï¸ Running on CPU with float32 - performance will be limited
INFO:     Application startup complete.
```

## Key Improvements âœ…

### **1. Dependency Management**
- âœ… **Proper accelerate installation** with flexible versioning
- âœ… **Removed unnecessary dependencies** for basic CPU loading
- âœ… **Cleaner dependency chain** with fewer requirements

### **2. Model Loading Strategy**
- âœ… **Simplified loading process** without device_map
- âœ… **Explicit device placement** for better control
- âœ… **Better error handling** with fewer failure points

### **3. Environment Configuration**
- âœ… **Modern HF_HOME usage** instead of deprecated TRANSFORMERS_CACHE
- âœ… **Cleaner environment variables** without warnings
- âœ… **Better cache management** with current best practices

### **4. Compatibility**
- âœ… **Works with or without accelerate** library
- âœ… **Compatible across different environments** (inf2.xlarge, inf2.8xlarge)
- âœ… **Reduced external dependencies** for basic functionality

## Testing Instructions

### **1. Rebuild the Image**:
```bash
cd images/neuron-dlc
./build.sh
```

### **2. Deploy and Monitor**:
```bash
kubectl apply -f kubernetes-deployment.yaml
kubectl logs -l app=neuron-mistral-7b-dlc -f
```

### **3. Expected Success Logs**:
```
INFO:     Started server process [8]
INFO:     Waiting for application startup.
âœ… Tokenizer loaded successfully (standard method)
âœ… Model loaded on CPU successfully
âœ… CPU fallback model loaded successfully (float32)
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000
```

### **4. Test Generation**:
```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Hello, I am",
    "max_tokens": 50
  }'
```

### **5. Expected Response**:
```json
{
  "text": "Hello, I am a helpful AI assistant...",
  "prompt": "Hello, I am",
  "model": "mistralai/Mistral-7B-Instruct-v0.3",
  "usage": {
    "prompt_tokens": 8,
    "completion_tokens": 25,
    "total_tokens": 33
  }
}
```

## Summary âœ…

The accelerate dependency issue has been resolved with:

1. âœ… **Updated accelerate installation** with flexible versioning
2. âœ… **Removed device_map dependency** for simpler CPU loading
3. âœ… **Explicit device placement** with `.to('cpu')`
4. âœ… **Removed deprecated environment variables** 
5. âœ… **Simplified dependency chain** for better compatibility

**ğŸ‰ The neuron-dlc container should now start successfully without accelerate dependency errors and provide a working text generation service on CPU!**
