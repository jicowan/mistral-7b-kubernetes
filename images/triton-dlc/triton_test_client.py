#!/usr/bin/env python3
"""
Test client for Triton-compatible vLLM server with AWS DLC
Tests both Triton-style and legacy endpoints
"""

import requests
import json
import time
import numpy as np
from typing import Dict, Any

class TritonCompatibleClient:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url.rstrip('/')
    
    def health_ready(self) -> bool:
        """Check if server is ready (Triton format)"""
        try:
            response = requests.get(f"{self.base_url}/v2/health/ready")
            return response.status_code == 200 and response.json().get("ready", False)
        except:
            return False
    
    def health_live(self) -> bool:
        """Check if server is live (Triton format)"""
        try:
            response = requests.get(f"{self.base_url}/v2/health/live")
            return response.status_code == 200 and response.json().get("live", False)
        except:
            return False
    
    def list_models(self) -> Dict[str, Any]:
        """List available models (Triton format)"""
        response = requests.get(f"{self.base_url}/v2/models")
        response.raise_for_status()
        return response.json()
    
    def get_model_metadata(self, model_name: str = "vllm_mistral") -> Dict[str, Any]:
        """Get model metadata (Triton format)"""
        response = requests.get(f"{self.base_url}/v2/models/{model_name}")
        response.raise_for_status()
        return response.json()
    
    def get_server_metadata(self) -> Dict[str, Any]:
        """Get server metadata (Triton format)"""
        response = requests.get(f"{self.base_url}/v2")
        response.raise_for_status()
        return response.json()
    
    def infer_triton(self, model_name: str, prompt: str, max_tokens: int = 512, 
                    temperature: float = 0.7, top_p: float = 0.9) -> Dict[str, Any]:
        """Run inference using Triton format"""
        
        # Prepare Triton-style request
        request_data = {
            "inputs": [
                {
                    "name": "prompt",
                    "datatype": "BYTES",
                    "shape": [1],
                    "data": [prompt]
                },
                {
                    "name": "max_tokens",
                    "datatype": "INT32",
                    "shape": [1],
                    "data": [max_tokens]
                },
                {
                    "name": "temperature",
                    "datatype": "FP32",
                    "shape": [1],
                    "data": [temperature]
                },
                {
                    "name": "top_p",
                    "datatype": "FP32",
                    "shape": [1],
                    "data": [top_p]
                }
            ],
            "outputs": [
                {
                    "name": "generated_text"
                }
            ]
        }
        
        response = requests.post(
            f"{self.base_url}/v2/models/{model_name}/infer",
            json=request_data,
            headers={"Content-Type": "application/json"}
        )
        response.raise_for_status()
        return response.json()

def main():
    # Initialize client
    client = TritonCompatibleClient()
    
    print("üöÄ Testing Triton-Compatible vLLM Server with AWS DLC")
    print("=" * 60)
    
    try:
        # Health checks
        print("1. Health Checks...")
        ready = client.health_ready()
        live = client.health_live()
        print(f"   Ready: {'‚úÖ' if ready else '‚ùå'}")
        print(f"   Live: {'‚úÖ' if live else '‚ùå'}")
        
        if not ready:
            print("‚ùå Server not ready. Exiting.")
            return
        
        # Server metadata
        print("\n2. Server Metadata...")
        server_meta = client.get_server_metadata()
        print(f"   Name: {server_meta.get('name', 'N/A')}")
        print(f"   Version: {server_meta.get('version', 'N/A')}")
        print(f"   Extensions: {server_meta.get('extensions', [])}")
        
        # List models
        print("\n3. Available Models...")
        models = client.list_models()
        for model in models.get('models', []):
            print(f"   - {model['name']} (version: {model['version']}, state: {model['state']})")
        
        # Model metadata
        print("\n4. Model Metadata...")
        model_meta = client.get_model_metadata("vllm_mistral")
        print(f"   Name: {model_meta['name']}")
        print(f"   Platform: {model_meta['platform']}")
        print(f"   Versions: {model_meta['versions']}")
        print(f"   Inputs: {len(model_meta['inputs'])} parameters")
        print(f"   Outputs: {len(model_meta['outputs'])} parameters")
        
        # Test inference with Triton format
        print("\n5. Triton-Style Inference Tests...")
        test_prompts = [
            "What is the capital of France?",
            "Explain machine learning in simple terms.",
            "Write a Python function to calculate factorial.",
            "What are the benefits of using AWS Deep Learning Containers?"
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n   Test {i}: {prompt}")
            print("   " + "-" * 50)
            
            start_time = time.time()
            result = client.infer_triton(
                model_name="vllm_mistral",
                prompt=prompt,
                max_tokens=256,
                temperature=0.7,
                top_p=0.9
            )
            end_time = time.time()
            
            # Extract generated text from Triton response
            generated_text = ""
            for output in result.get('outputs', []):
                if output['name'] == 'generated_text':
                    data = output['data'][0]
                    if isinstance(data, bytes):
                        generated_text = data.decode('utf-8')
                    else:
                        generated_text = str(data)
                    break
            
            print(f"   Response: {generated_text[:200]}...")
            print(f"   Time: {end_time - start_time:.2f}s")
            print(f"   Model: {result.get('model_name', 'N/A')}")
            print(f"   Version: {result.get('model_version', 'N/A')}")
        
        print("\n‚úÖ All Triton-compatible tests completed successfully!")
        
        # Performance summary
        print("\nüìä Performance Summary:")
        print("   - Triton-compatible API: ‚úÖ Working")
        print("   - Health endpoints: ‚úÖ Working")
        print("   - Model management: ‚úÖ Working")
        print("   - Inference pipeline: ‚úÖ Working")
        print("   - AWS DLC optimizations: ‚úÖ Active")
        
        print("\nüèóÔ∏è  Architecture Benefits:")
        print("   - AWS DLC base image for optimizations")
        print("   - vLLM backend for high performance")
        print("   - Triton-compatible API for standardization")
        print("   - CUDA graphs for GPU efficiency")
        print("   - Dynamic batching capability")
        
    except requests.exceptions.ConnectionError:
        print("‚ùå Connection failed. Make sure the server is running.")
        print("   Start with: kubectl port-forward service/triton-mistral-7b-dlc-service 8000:8000")
    except requests.exceptions.HTTPError as e:
        print(f"‚ùå HTTP error: {e}")
        if e.response.status_code == 503:
            print("   Server may still be initializing. Please wait and try again.")
    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")

if __name__ == "__main__":
    main()
